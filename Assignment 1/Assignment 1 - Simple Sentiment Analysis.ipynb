{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7078,"status":"ok","timestamp":1709746891316,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"cD11k9FQIxzW","outputId":"827ac2dc-28e1-4054-9b4a-b132db7b53cb"},"outputs":[],"source":["#pip install torchtext==0.4.0"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8450,"status":"ok","timestamp":1709746899764,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"diXAJf2jH1tQ"},"outputs":[],"source":["import torch\n","from torchtext import data\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","#TEXT used to store text, and LABEL used to store labels\n","TEXT = data.Field(tokenize = 'spacy',   #Tokenizer\n","                  tokenizer_language = 'en_core_web_sm',\n","                  include_lengths = True,   #includes length of text in the field\n","                  pad_first=True)   #Pad the start and end of text when it is shorter than max length\n","LABEL = data.LabelField(dtype = torch.float)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77102,"status":"ok","timestamp":1709746976859,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"nOTiebiKH1tR","outputId":"36db01b2-40d5-4567-d3c2-89e75c3fb357"},"outputs":[],"source":["from torchtext import datasets\n","\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)   #Process the dataset in the way defined in TEXT and LABEL"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["25000\n","17500\n"]}],"source":["print(len(test_data))\n","print(len(train_data))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Maximum sentence length: 2789\n"]}],"source":["max_len = 0\n","for sample in train_data:\n","  text_len = len(sample.text) # Access the second element (length)\n","  \n","  max_len = max(max_len, text_len)\n","\n","print(\"Maximum sentence length:\", max_len)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1709746976860,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"eCXoOIyVH1tS"},"outputs":[],"source":["import random\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":862,"status":"ok","timestamp":1709746977702,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"Kzq6r-FtH1tS"},"outputs":[],"source":["MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n","LABEL.build_vocab(train_data)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1709746977703,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"5QYGHCMfH1tT","outputId":"e98cf267-ebd9-42eb-ebbd-91e7f9016dd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(device)\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    device = device)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[torchtext.data.batch.Batch of size 64]\n","\t[.text]:('[torch.cuda.LongTensor of size 169x64 (GPU 0)]', '[torch.cuda.LongTensor of size 64 (GPU 0)]')\n","\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n"]}],"source":["for i, k in enumerate(train_iterator):\n","    if i == 0:\n","        print(k)\n","    else:\n","        break"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"]}],"source":["print(TEXT.vocab.itos[:10])"]},{"cell_type":"markdown","metadata":{"id":"UwOvT5bEHyJn"},"source":["## Initial Architecture"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1709747736070,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"Q5IWVRASH1tT"},"outputs":[],"source":["#No Word2Vec\n","import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, text, text_lengths):\n","        #text = [sent len, batch size]\n","\n","        embedded = self.embedding(text)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        output, hidden = self.rnn(embedded)\n","        #output = [sent len, batch size, hid dim]\n","        #hidden = [1, batch size, hid dim]\n","\n","\n","        #assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n","\n","        return self.fc(hidden.squeeze(0))"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":402,"status":"ok","timestamp":1709747739360,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"C-NMrSMCH1tT"},"outputs":[],"source":["#For non-word2Vec\n","INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","\n","model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"]},{"cell_type":"markdown","metadata":{"id":"fJuSiQMmrZOl"},"source":["## Word2vec"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"FuC5CzCi_d58"},"outputs":[],"source":["import gensim.models.keyedvectors as word2vec\n","\n","wv_model = word2vec.KeyedVectors.load_word2vec_format(\n","    \"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709616982896,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"MHFp-4Qtl-51","outputId":"294a3f38-a0b6-4931-9f44-c85b1b8dcfc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary size: 3000000\n","Vector dimension: 300\n"]}],"source":["vocab_size = len(wv_model)\n","vector_dimension = wv_model.vector_size\n","\n","print(f\"Vocabulary size: {vocab_size}\")\n","print(f\"Vector dimension: {vector_dimension}\")\n"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['antics.<br', 'any.<br', 'approach.<br', '<unk>']\n"]}],"source":["miss = []\n","for i in range(-10, 1):\n","    text = TEXT.vocab.itos[i]\n","\n","    if text not in wv_model.key_to_index:\n","        miss.append(text)\n","print(miss)\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"ylYlTPkVySZb"},"outputs":[{"name":"stdout","output_type":"stream","text":["missed words = 1846; found words = 25002\n"]},{"data":{"text/plain":["Embedding(25002, 300)"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["#Match TEXT and W2V Embedding\n","#Non-matching word to unk\n","\n","filtered_embedding = []\n","missed_word = 0\n","for i in range(len(TEXT.vocab)):\n","  text = TEXT.vocab.itos[i]\n","\n","  if text in wv_model.key_to_index:\n","    text_embedding = wv_model[text]\n","    filtered_embedding.append(text_embedding)\n","  else:\n","    missed_word += 1\n","    text_embedding = wv_model['unk']\n","    filtered_embedding.append(text_embedding)\n","    continue\n","\n","print(f\"missed words = {missed_word}; found words = {len(filtered_embedding)}\")\n","\n","\n","#Convert embedding to tensor\n","from torch.nn import Embedding\n","import numpy as np\n","\n","embedding_weights = torch.FloatTensor(np.array(filtered_embedding))\n","#embedding = Embedding.from_pretrained(embedding_weights).to(device)\n","embedding = Embedding.from_pretrained(embedding_weights, freeze = False).to(device)\n","embedding"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"g-beXrlDGtdS"},"outputs":[],"source":["#Match TEXT and W2V Embedding\n","#Non-matching word to mean and std w2v embedding\n","\n","import numpy as np\n","from torch.nn import Embedding\n","\n","embeddings = np.array([wv_model[wv_model.index_to_key[i]] for i in range(len(wv_model))])\n","mean = np.mean(embeddings, axis=0)\n","std = np.std(embeddings, axis=0)\n","\n","# Pre-process OOV embeddings\n","oov_embeddings = {}\n","for text in TEXT.vocab.itos:\n","    if text not in wv_model.key_to_index:\n","        oov_embeddings[text] = np.random.normal(loc=mean, scale=std, size=300)\n","\n","filtered_embedding = np.empty((len(TEXT.vocab), 300))\n","\n","for i in range(len(TEXT.vocab)):\n","    text = TEXT.vocab.itos[i]\n","\n","    if text in wv_model.key_to_index:\n","        text_embedding = wv_model[text]\n","    else:\n","        text_embedding = oov_embeddings[text]\n","\n","    filtered_embedding[i] = text_embedding\n","\n","embedding_weights = torch.FloatTensor(filtered_embedding)\n","#embedding = Embedding.from_pretrained(embedding_weights).to(device) #Frozen weight\n","embedding = Embedding.from_pretrained(embedding_weights, freeze = False).to(device) #Unfrozen weight\n"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["#OOV words sampled from mean and std of vocab embedding\n","import numpy as np\n","from torch.nn import Embedding\n","\n","available_embeddings = []\n","for i in range(len(TEXT.vocab)):\n","    text = TEXT.vocab.itos[i]\n","\n","    if text in wv_model.key_to_index:\n","        available_embeddings.append(wv_model[text])\n","\n","available_embeddings = np.array(available_embeddings)\n","mean = np.mean(available_embeddings, axis = 0)\n","std = np.std(available_embeddings, axis = 0 )\n","\n","oov_embeddings = {}\n","for text in TEXT.vocab.itos:\n","    if text not in wv_model.key_to_index:\n","        oov_embeddings[text] = np.random.normal(loc=mean, scale=std, size=300)\n","\n","filtered_embedding = np.empty((len(TEXT.vocab), 300))\n","\n","for i in range(len(TEXT.vocab)):\n","    text = TEXT.vocab.itos[i]\n","\n","    if text in wv_model.key_to_index:\n","        text_embedding = wv_model[text]\n","    else:\n","        text_embedding = oov_embeddings[text]\n","\n","    filtered_embedding[i] = text_embedding\n","\n","embedding_weights = torch.FloatTensor(filtered_embedding)\n","embedding = Embedding.from_pretrained(embedding_weights).to(device) #Frozen weight"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"W0MiHOfo4f4R"},"outputs":[],"source":["#With Word2Vec\n","import torch.nn as nn\n","from torch.nn import Embedding\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_dim, embedding, hidden_dim, output_dim):\n","\n","        super().__init__()\n","\n","        self.embedding = embedding\n","        \n","\n","        self.rnn = nn.RNN(300, hidden_dim)\n","\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, text, text_lengths):\n","        #text = [sent len, batch size]\n","\n","        embedded = self.embedding(text)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        output, hidden = self.rnn(embedded)\n","        #output = [sent len, batch size, hid dim]\n","        #hidden = [1, batch size, hid dim]\n","\n","        #assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n","\n","        return self.fc(hidden.squeeze(0))"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"OnTGz5kLsNT7"},"outputs":[],"source":["#For word2vec\n","INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING = embedding\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","\n","model = RNN(INPUT_DIM, EMBEDDING, HIDDEN_DIM, OUTPUT_DIM)"]},{"cell_type":"markdown","metadata":{"id":"Xp5Di_-x9KRq"},"source":["# Different models\n","\n","Use Adam optimizer, 50 epochs and randomly initialized embeddings, run the\n","experiments with the following models:\n","\n","1. One-layer feed forward neural network, hidden dimension is 500.\n","2. Two-layer feed forward neural network, hidden dimensions are 500 and 300.\n","3. Three-layer feed forward neural network, hidden dimensions are 500, 300, and\n","200\n","4. CNN model (using three feature maps with the sizes are 1, 2, and 3)\n","5. LSTM model\n","6. Bi-LSTM model"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Zix2iIVpD2oV"},"outputs":[],"source":["#One-layer FFN: hidden dim = 500    \n","import torch.nn as nn\n","\n","class FFN1(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","\n","        super(FFN1, self).__init__()\n","\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","\n","        self.output_dim = output_dim\n","\n","        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n","\n","        self.output = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, text, text_lengths):\n","        #text = [sent len, batch size]\n","\n","        embedded = self.embedding(text)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        x = self.fc1(embedded)\n","\n","        x = nn.functional.relu(x) #ReLU activation\n","\n","        x = torch.mean(x, dim=0) \n","        \n","        output = self.output(x)\n","\n","        #assert torch.equal(output[-1,:,:], x.squeeze(0))\n","\n","        return output.squeeze()"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["#Two-layer FFN: hidden dim = 500, 300   \n","import torch.nn as nn\n","\n","class FFN2(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim1, hidden_dim2, output_dim):\n","\n","        super(FFN2, self).__init__()\n","\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","\n","        self.output_dim = output_dim\n","\n","        self.fc1 = nn.Linear(embedding_dim, hidden_dim1)\n","\n","        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n","\n","        self.output = nn.Linear(hidden_dim2, output_dim)\n","\n","    def forward(self, text, text_lengths):\n","        #text = [sent len, batch size]\n","\n","        embedded = self.embedding(text)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        x = self.fc1(embedded)\n","        x = nn.functional.relu(x) #ReLU activation\n","\n","        x = self.fc2(x)\n","        x = nn.functional.relu(x)\n","\n","        x = torch.mean(x, dim=0) \n","        \n","        \n","        output = self.output(x)\n","\n","        #assert torch.equal(output[-1,:,:], x.squeeze(0))\n","\n","        return output.squeeze()"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["#Three-layer FFN: hidden dim = 500, 300, 200\n","import torch.nn as nn\n","\n","class FFN3(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n","\n","        super(FFN3, self).__init__()\n","\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","\n","        self.output_dim = output_dim\n","\n","        self.fc1 = nn.Linear(embedding_dim, hidden_dim1)\n","\n","        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n","\n","        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n","\n","        self.output = nn.Linear(hidden_dim3, output_dim)\n","\n","    def forward(self, text, text_lengths):\n","        #text = [sent len, batch size]\n","\n","        embedded = self.embedding(text)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        x = self.fc1(embedded)\n","        x = nn.functional.relu(x) #ReLU activation\n","\n","        x = self.fc2(x)\n","        x = nn.functional.relu(x)\n","\n","        x = self.fc3(x)\n","        x = nn.functional.relu(x)\n","\n","        x = torch.mean(x, dim=0) \n","    \n","        output = self.output(x)\n","\n","        #assert torch.equal(output[-1,:,:], x.squeeze(0))\n","\n","        return output.squeeze()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["#CNN using Conv2d\n","import torch.nn as nn\n","\n","class CNN(nn.Module):\n","  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","    super(CNN, self).__init__()\n","    self.embedding = nn.Embedding(input_dim, embedding_dim)  \n","    self.conv1 = nn.Conv2d(1, hidden_dim, kernel_size=(1, embedding_dim))  \n","    self.conv2 = nn.Conv2d(1, hidden_dim, kernel_size=(2, embedding_dim))  \n","    self.conv3 = nn.Conv2d(1, hidden_dim, kernel_size=(3, embedding_dim))  \n","    self.relu = nn.ReLU()\n","    self.pool = nn.AdaptiveMaxPool2d(1)  \n","    self.output = nn.Linear(hidden_dim * 3, output_dim)  \n","  \n","\n","  def forward(self, text, text_lengths):\n","    \n","    embedded = self.embedding(text)  \n","    embedded = embedded.permute(1,0,2)\n","    embedded = embedded.unsqueeze(1)  # Add a channel dimension (required for Conv2d)\n","\n","    conv1 = self.relu(self.conv1(embedded))\n","    conv2 = self.relu(self.conv2(embedded))\n","    conv3 = self.relu(self.conv3(embedded))\n","\n","    \n","    pooled1 = self.pool(conv1)\n","    pooled2 = self.pool(conv2)\n","    pooled3 = self.pool(conv3)\n","    \n","\n","    features = torch.cat([pooled1, pooled2, pooled3], dim=1).squeeze()\n","    output = self.output(features)  \n","\n","    return output.squeeze()"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["#CNN (Alternative)\n","import torch.nn as nn \n","\n","class CNN(nn.Module):\n","  def __init__(self, input_dim, embedding_dim, kernel_sizes:list, hidden_dim, output_dim):\n","    super(CNN, self).__init__()\n","    self.embedding = nn.Embedding(input_dim, embedding_dim)\n","    self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, hidden_dim, kernel_size=k) for k in kernel_sizes])\n","    self.pool = nn.AdaptiveMaxPool1d(1)\n","    self.relu = nn.ReLU()  # Add ReLU layer after each convolution\n","    self.fc = nn.Linear(len(kernel_sizes) * hidden_dim, output_dim)\n","\n","  def forward(self, text, text_lengths):\n","    embedded = self.embedding(text)\n","    embedded = embedded.permute(1, 2, 0)\n","    x = torch.cat([self.relu(self.pool(conv(embedded))) for conv in self.convs], dim=1)\n","    #x = torch.cat([(self.pool(conv(embedded))) for conv in self.convs], dim=1)\n","    x = x.squeeze(2)\n","    output = self.fc(x)\n","    return output\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["#LSTM\n","import torch.nn as nn\n","class LSTM(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","        super(LSTM, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n","        self.output = nn.Linear(hidden_dim, output_dim)\n","\n","\n","    def forward(self, text, text_lengths):\n","        embedded = self.embedding(text)\n","        _, (hidden,cell) = self.lstm(embedded) #no need for activation because of gating mechanism\n","        output = self.output(hidden)\n","        \n","\n","        return output.squeeze()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["#BILSTM\n","import torch.nn as nn\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional= True)\n","        self.output = nn.Linear(hidden_dim*2, output_dim)\n","\n","\n","    def forward(self, text, text_lengths):\n","        embedded = self.embedding(text)\n","        _, (hidden,cell) = self.bilstm(embedded) #no need for activation because of gating mechanism\n","        hidden = torch.concat((hidden[0,:,:], hidden[1,:,:]), dim = 1)       #2 hidden states, concat them together along batch size\n","        #print('hidden:' , hidden.shape)\n","        \n","        output = self.output(hidden)\n","        #print('output: ', output.shape)\n","        \n","\n","        return output.squeeze()"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"dx4k4dMNJr0i"},"outputs":[],"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256 \n","OUTPUT_DIM = 1\n","\n","model = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM,  OUTPUT_DIM).to(device)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 10\n","OUTPUT_DIM = 1\n","\n","#model = FFN1(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model = CNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"]},{"cell_type":"markdown","metadata":{"id":"wrkr-KlaH1tU"},"source":["# Train the Model"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"wcSM1EMeH1tU"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has 2,592,105 trainable parameters\n","The model has 2,592,105 total parameters\n","RNN(\n","  (embedding): Embedding(25002, 100)\n","  (rnn): RNN(100, 256)\n","  (fc): Linear(in_features=256, out_features=1, bias=True)\n",")\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","def count_allparams(model):\n","    return sum(p.numel() for p in model.parameters())\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')\n","print(f'The model has {count_allparams(model):,} total parameters')\n","print(model)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":477,"status":"ok","timestamp":1709747745733,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"GvMIccglH1tU"},"outputs":[],"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","#optimizer = optim.SGD(model.parameters(), lr=1e-3)\n","#optimizer = optim.Adagrad(model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709747748340,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"Y24HzSVUH1tU"},"outputs":[],"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","#there is another type of loss: nn.BCELoss. This assumes we manually calculate the sigmoid prior."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":285,"status":"ok","timestamp":1709747472613,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"Oxpv2NL9H1tU"},"outputs":[],"source":["model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1709747469647,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"wp5qxRMeH1tU"},"outputs":[],"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division\n","    acc = correct.sum() / len(correct)\n","    return acc"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":502,"status":"ok","timestamp":1709747435358,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"nqCbCqPuH1tU"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","\n","    for batch in iterator:\n","\n","        optimizer.zero_grad()\n","\n","        text, text_lengths = batch.text\n","\n","        predictions = model(text, text_lengths).squeeze()       #Initial: squeeze(1)\n","\n","        loss = criterion(predictions, batch.label)\n","\n","        acc = binary_accuracy(predictions, batch.label)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","        epoch_acc += acc.item()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709747436752,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"eS3rZWzhH1tU"},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for batch in iterator:\n","            text, text_lengths = batch.text\n","\n","            predictions = model(text, text_lengths).squeeze()   #Initial: squeeze(1)\n","\n","            loss = criterion(predictions, batch.label)\n","\n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":655,"status":"ok","timestamp":1709747438858,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"EzkEdOh_H1tU"},"outputs":[],"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48103,"status":"ok","timestamp":1709747720935,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"O4gqfpz4wGxR"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.691 | Train Acc: 53.96%\n","\t Val. Loss: 0.691 |  Val. Acc: 52.87%\n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.690 | Train Acc: 53.19%\n","\t Val. Loss: 0.683 |  Val. Acc: 54.35%\n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.682 | Train Acc: 54.52%\n","\t Val. Loss: 0.678 |  Val. Acc: 55.36%\n","Epoch: 04 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.655 | Train Acc: 60.29%\n","\t Val. Loss: 0.651 |  Val. Acc: 62.24%\n","Epoch: 05 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.595 | Train Acc: 68.07%\n","\t Val. Loss: 0.680 |  Val. Acc: 54.07%\n","Epoch: 06 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.564 | Train Acc: 71.43%\n","\t Val. Loss: 0.600 |  Val. Acc: 66.79%\n","Epoch: 07 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.538 | Train Acc: 73.25%\n","\t Val. Loss: 0.576 |  Val. Acc: 70.72%\n","Epoch: 08 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.500 | Train Acc: 76.39%\n","\t Val. Loss: 0.579 |  Val. Acc: 71.40%\n","Epoch: 09 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.441 | Train Acc: 80.74%\n","\t Val. Loss: 0.536 |  Val. Acc: 74.93%\n","Epoch: 10 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.498 | Train Acc: 76.60%\n","\t Val. Loss: 0.611 |  Val. Acc: 72.55%\n","Epoch: 11 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.404 | Train Acc: 83.37%\n","\t Val. Loss: 0.583 |  Val. Acc: 75.02%\n","Epoch: 12 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.469 | Train Acc: 77.03%\n","\t Val. Loss: 0.603 |  Val. Acc: 73.88%\n","Epoch: 13 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.368 | Train Acc: 85.56%\n","\t Val. Loss: 0.622 |  Val. Acc: 73.23%\n","Epoch: 14 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.471 | Train Acc: 77.27%\n","\t Val. Loss: 0.632 |  Val. Acc: 72.35%\n","Epoch: 15 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.354 | Train Acc: 86.24%\n","\t Val. Loss: 0.586 |  Val. Acc: 72.07%\n","Epoch: 16 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.313 | Train Acc: 88.26%\n","\t Val. Loss: 0.586 |  Val. Acc: 74.55%\n","Epoch: 17 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.318 | Train Acc: 87.59%\n","\t Val. Loss: 0.590 |  Val. Acc: 70.88%\n","Epoch: 18 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.324 | Train Acc: 87.70%\n","\t Val. Loss: 0.583 |  Val. Acc: 74.41%\n","Epoch: 19 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.255 | Train Acc: 90.68%\n","\t Val. Loss: 0.619 |  Val. Acc: 75.62%\n","Epoch: 20 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.288 | Train Acc: 89.47%\n","\t Val. Loss: 0.674 |  Val. Acc: 73.69%\n","Test Loss: 0.554 | Test Acc: 73.71%\n"]}],"source":["#Original; also for Word2vec\n","\n","N_EPOCHS = 20\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut1-model.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","model.load_state_dict(torch.load('tut1-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":85795,"status":"error","timestamp":1709743799924,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"FnMUXr4Pdzm9","outputId":"f91a7fbe-5af6-407a-9996-84be9287ff56"},"outputs":[{"name":"stdout","output_type":"stream","text":["N_EPOCHS is 50\n","Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.685 | Train Acc: 54.71%\n","\t Val. Loss: 0.674 |  Val. Acc: 58.34%\n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.651 | Train Acc: 62.04%\n","\t Val. Loss: 0.658 |  Val. Acc: 60.69%\n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.627 | Train Acc: 65.01%\n","\t Val. Loss: 0.632 |  Val. Acc: 64.82%\n","Epoch: 04 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.605 | Train Acc: 67.72%\n","\t Val. Loss: 0.613 |  Val. Acc: 66.70%\n","Epoch: 05 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.595 | Train Acc: 68.60%\n","\t Val. Loss: 0.606 |  Val. Acc: 67.46%\n","Epoch: 06 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.587 | Train Acc: 69.27%\n","\t Val. Loss: 0.600 |  Val. Acc: 67.47%\n","Epoch: 07 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.581 | Train Acc: 69.89%\n","\t Val. Loss: 0.594 |  Val. Acc: 69.00%\n","Epoch: 08 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.580 | Train Acc: 69.83%\n","\t Val. Loss: 0.599 |  Val. Acc: 67.76%\n","Epoch: 09 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.572 | Train Acc: 70.91%\n","\t Val. Loss: 0.584 |  Val. Acc: 69.92%\n","Epoch: 10 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.572 | Train Acc: 70.53%\n","\t Val. Loss: 0.624 |  Val. Acc: 64.86%\n","Epoch: 11 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.568 | Train Acc: 71.43%\n","\t Val. Loss: 0.580 |  Val. Acc: 69.83%\n","Epoch: 12 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.554 | Train Acc: 72.50%\n","\t Val. Loss: 0.576 |  Val. Acc: 70.11%\n","Epoch: 13 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.552 | Train Acc: 72.51%\n","\t Val. Loss: 0.576 |  Val. Acc: 70.52%\n","Epoch: 14 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.547 | Train Acc: 72.96%\n","\t Val. Loss: 0.573 |  Val. Acc: 70.69%\n","Epoch: 15 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.542 | Train Acc: 73.39%\n","\t Val. Loss: 0.564 |  Val. Acc: 71.55%\n","Epoch: 16 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.537 | Train Acc: 73.79%\n","\t Val. Loss: 0.562 |  Val. Acc: 71.51%\n","Epoch: 17 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.533 | Train Acc: 74.00%\n","\t Val. Loss: 0.559 |  Val. Acc: 71.72%\n","Epoch: 18 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.536 | Train Acc: 74.05%\n","\t Val. Loss: 0.559 |  Val. Acc: 71.80%\n","Epoch: 19 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.526 | Train Acc: 74.48%\n","\t Val. Loss: 0.558 |  Val. Acc: 71.92%\n","Epoch: 20 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.524 | Train Acc: 74.84%\n","\t Val. Loss: 0.553 |  Val. Acc: 72.90%\n","Epoch: 21 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.521 | Train Acc: 75.03%\n","\t Val. Loss: 0.550 |  Val. Acc: 73.05%\n","Epoch: 22 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.518 | Train Acc: 75.26%\n","\t Val. Loss: 0.548 |  Val. Acc: 72.70%\n","Epoch: 23 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.523 | Train Acc: 74.55%\n","\t Val. Loss: 0.548 |  Val. Acc: 73.16%\n","Epoch: 24 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.528 | Train Acc: 74.05%\n","\t Val. Loss: 0.547 |  Val. Acc: 72.90%\n","Epoch: 25 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.517 | Train Acc: 75.10%\n","\t Val. Loss: 0.546 |  Val. Acc: 73.19%\n","Epoch: 26 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.515 | Train Acc: 75.28%\n","\t Val. Loss: 0.542 |  Val. Acc: 72.94%\n","Epoch: 27 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.507 | Train Acc: 75.98%\n","\t Val. Loss: 0.541 |  Val. Acc: 73.21%\n","Epoch: 28 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.508 | Train Acc: 75.93%\n","\t Val. Loss: 0.541 |  Val. Acc: 73.27%\n","Epoch: 29 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.500 | Train Acc: 76.30%\n","\t Val. Loss: 0.535 |  Val. Acc: 73.83%\n","Epoch: 30 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.496 | Train Acc: 76.80%\n","\t Val. Loss: 0.532 |  Val. Acc: 74.40%\n","Epoch: 31 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.495 | Train Acc: 76.90%\n","\t Val. Loss: 0.529 |  Val. Acc: 74.69%\n","Epoch: 32 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.491 | Train Acc: 77.15%\n","\t Val. Loss: 0.527 |  Val. Acc: 74.77%\n","Epoch: 33 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.488 | Train Acc: 77.17%\n","\t Val. Loss: 0.529 |  Val. Acc: 74.44%\n","Epoch: 34 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.488 | Train Acc: 77.29%\n","\t Val. Loss: 0.525 |  Val. Acc: 74.62%\n","Epoch: 35 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.485 | Train Acc: 77.54%\n","\t Val. Loss: 0.523 |  Val. Acc: 74.41%\n","Epoch: 36 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.481 | Train Acc: 77.93%\n","\t Val. Loss: 0.520 |  Val. Acc: 75.01%\n","Epoch: 37 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.482 | Train Acc: 77.79%\n","\t Val. Loss: 0.520 |  Val. Acc: 74.90%\n","Epoch: 38 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.481 | Train Acc: 77.90%\n","\t Val. Loss: 0.517 |  Val. Acc: 75.45%\n","Epoch: 39 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.476 | Train Acc: 78.23%\n","\t Val. Loss: 0.520 |  Val. Acc: 75.06%\n","Epoch: 40 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.473 | Train Acc: 78.24%\n","\t Val. Loss: 0.514 |  Val. Acc: 75.42%\n","Epoch: 41 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.470 | Train Acc: 78.58%\n","\t Val. Loss: 0.512 |  Val. Acc: 75.74%\n","Epoch: 42 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.468 | Train Acc: 78.74%\n","\t Val. Loss: 0.513 |  Val. Acc: 75.59%\n","Epoch: 43 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.465 | Train Acc: 78.83%\n","\t Val. Loss: 0.511 |  Val. Acc: 75.86%\n","Epoch: 44 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.463 | Train Acc: 79.09%\n","\t Val. Loss: 0.510 |  Val. Acc: 75.46%\n","Epoch: 45 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.463 | Train Acc: 79.03%\n","\t Val. Loss: 0.506 |  Val. Acc: 75.87%\n","Epoch: 46 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.459 | Train Acc: 79.35%\n","\t Val. Loss: 0.512 |  Val. Acc: 75.57%\n","Epoch: 47 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.457 | Train Acc: 79.45%\n","\t Val. Loss: 0.503 |  Val. Acc: 76.00%\n","Epoch: 48 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.455 | Train Acc: 79.48%\n","\t Val. Loss: 0.501 |  Val. Acc: 76.27%\n","Epoch: 49 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.453 | Train Acc: 79.51%\n","\t Val. Loss: 0.499 |  Val. Acc: 76.35%\n","Epoch: 50 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.450 | Train Acc: 79.61%\n","\t Val. Loss: 0.498 |  Val. Acc: 76.23%\n","Test Loss: 0.523 | Test Acc: 74.66%\n","N_EPOCHS is 20\n","Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.686 | Train Acc: 53.94%\n","\t Val. Loss: 0.678 |  Val. Acc: 56.74%\n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.678 | Train Acc: 58.24%\n","\t Val. Loss: 0.688 |  Val. Acc: 54.29%\n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.682 | Train Acc: 56.74%\n","\t Val. Loss: 0.682 |  Val. Acc: 55.55%\n","Epoch: 04 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.678 | Train Acc: 57.35%\n","\t Val. Loss: 0.680 |  Val. Acc: 55.95%\n","Epoch: 05 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.675 | Train Acc: 58.04%\n","\t Val. Loss: 0.676 |  Val. Acc: 56.86%\n","Epoch: 06 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.651 | Train Acc: 62.88%\n","\t Val. Loss: 0.644 |  Val. Acc: 62.69%\n","Epoch: 07 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.629 | Train Acc: 65.31%\n","\t Val. Loss: 0.635 |  Val. Acc: 64.27%\n","Epoch: 08 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.619 | Train Acc: 66.00%\n","\t Val. Loss: 0.634 |  Val. Acc: 64.34%\n","Epoch: 09 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.611 | Train Acc: 66.79%\n","\t Val. Loss: 0.624 |  Val. Acc: 65.51%\n","Epoch: 10 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.602 | Train Acc: 68.05%\n","\t Val. Loss: 0.621 |  Val. Acc: 65.68%\n","Epoch: 11 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.596 | Train Acc: 68.31%\n","\t Val. Loss: 0.620 |  Val. Acc: 65.80%\n","Epoch: 12 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.593 | Train Acc: 68.49%\n","\t Val. Loss: 0.613 |  Val. Acc: 67.14%\n","Epoch: 13 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.590 | Train Acc: 68.94%\n","\t Val. Loss: 0.610 |  Val. Acc: 67.32%\n","Epoch: 14 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.583 | Train Acc: 69.92%\n","\t Val. Loss: 0.605 |  Val. Acc: 68.05%\n","Epoch: 15 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.578 | Train Acc: 70.32%\n","\t Val. Loss: 0.603 |  Val. Acc: 67.78%\n","Epoch: 16 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.578 | Train Acc: 70.24%\n","\t Val. Loss: 0.601 |  Val. Acc: 68.15%\n","Epoch: 17 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.570 | Train Acc: 70.84%\n","\t Val. Loss: 0.605 |  Val. Acc: 67.91%\n","Epoch: 18 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.566 | Train Acc: 71.43%\n","\t Val. Loss: 0.597 |  Val. Acc: 68.73%\n","Epoch: 19 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.563 | Train Acc: 71.46%\n","\t Val. Loss: 0.598 |  Val. Acc: 68.60%\n","Epoch: 20 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.558 | Train Acc: 71.93%\n","\t Val. Loss: 0.593 |  Val. Acc: 68.93%\n","Test Loss: 0.591 | Test Acc: 69.02%\n","N_EPOCHS is 10\n","Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.686 | Train Acc: 54.60%\n","\t Val. Loss: 0.681 |  Val. Acc: 55.84%\n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.672 | Train Acc: 58.77%\n","\t Val. Loss: 0.676 |  Val. Acc: 57.09%\n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.650 | Train Acc: 62.17%\n","\t Val. Loss: 0.638 |  Val. Acc: 63.99%\n","Epoch: 04 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.633 | Train Acc: 64.06%\n","\t Val. Loss: 0.640 |  Val. Acc: 63.41%\n","Epoch: 05 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.624 | Train Acc: 65.43%\n","\t Val. Loss: 0.648 |  Val. Acc: 61.83%\n","Epoch: 06 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.612 | Train Acc: 66.69%\n","\t Val. Loss: 0.632 |  Val. Acc: 65.37%\n","Epoch: 07 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.605 | Train Acc: 67.10%\n","\t Val. Loss: 0.625 |  Val. Acc: 66.02%\n","Epoch: 08 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.598 | Train Acc: 67.81%\n","\t Val. Loss: 0.630 |  Val. Acc: 64.84%\n","Epoch: 09 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.591 | Train Acc: 68.58%\n","\t Val. Loss: 0.616 |  Val. Acc: 67.02%\n","Epoch: 10 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.590 | Train Acc: 68.72%\n","\t Val. Loss: 0.615 |  Val. Acc: 67.16%\n","Test Loss: 0.618 | Test Acc: 66.44%\n","N_EPOCHS is 5\n","Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.685 | Train Acc: 55.03%\n","\t Val. Loss: 0.674 |  Val. Acc: 57.15%\n","Epoch: 02 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.672 | Train Acc: 59.37%\n","\t Val. Loss: 0.673 |  Val. Acc: 56.42%\n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.646 | Train Acc: 62.60%\n","\t Val. Loss: 0.645 |  Val. Acc: 63.16%\n","Epoch: 04 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.670 | Train Acc: 58.65%\n","\t Val. Loss: 0.659 |  Val. Acc: 61.77%\n","Epoch: 05 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.671 | Train Acc: 58.71%\n","\t Val. Loss: 0.670 |  Val. Acc: 59.23%\n","Test Loss: 0.655 | Test Acc: 62.07%\n"]}],"source":["# MY own train (non-word2vec)\n","\n","#Define different hyperparameters\n","import torch.optim as optim\n","#epochs = [50]\n","epochs = [50, 20, 10, 5]\n","\n","def optimizer_to(optim, device):\n","    for param in optim.state.values():\n","        # Not sure there are any global tensors in the state dict\n","        if isinstance(param, torch.Tensor):\n","            param.data = param.data.to(device)\n","            if param._grad is not None:\n","                param._grad.data = param._grad.data.to(device)\n","        elif isinstance(param, dict):\n","            for subparam in param.values():\n","                if isinstance(subparam, torch.Tensor):\n","                    subparam.data = subparam.data.to(device)\n","                    if subparam._grad is not None:\n","                        subparam._grad.data = subparam._grad.data.to(device)\n","    return optim\n","\n","for N_EPOCHS in epochs:\n","  print(f'N_EPOCHS is {N_EPOCHS}')\n","  model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","  optimizer = optim.Adagrad(model.parameters(), lr= 1e-3)\n","  criterion = nn.BCEWithLogitsLoss()\n","  model = model.to(device)\n","  criterion = criterion.to(device)\n","  optimizer = optimizer_to(optimizer, device)\n","  best_valid_loss = float('inf')\n","\n","  for epoch in range(N_EPOCHS):\n","      start_time = time.time()\n","\n","      train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","      valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","\n","      end_time = time.time()\n","\n","      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","      if valid_loss < best_valid_loss:\n","          best_valid_loss = valid_loss\n","          torch.save(model.state_dict(), 'tut1-model.pt')\n","\n","      print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","      print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","  model.load_state_dict(torch.load('tut1-model.pt'))\n","\n","  test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAjehc9qH1tV"},"outputs":[],"source":["#My own train 2 (non-word2vec)\n","import torch.optim as optim\n","\n","N_EPOCHS = 20\n","optimizer = optim.Adagrad(model.parameters(), lr= 1e-3)\n","\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut1-model.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","\n","model.load_state_dict(torch.load('tut1-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","\n","model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","criterion = nn.BCEWithLogitsLoss()\n","model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3575,"status":"ok","timestamp":1709744164528,"user":{"displayName":"Alfred Tan","userId":"12418543825992472900"},"user_tz":-480},"id":"49vGHz2WH1tV","outputId":"28765cb6-48b9-4b96-b3b2-b889c1b4f4c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 0.562 | Test Acc: 75.85%\n"]}],"source":["model.load_state_dict(torch.load('tut1-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["UwOvT5bEHyJn"],"gpuType":"T4","name":"","provenance":[{"file_id":"1-zXNgxFIGeTKuw2s2ftBscCKYCUnF20n","timestamp":1708605453209},{"file_id":"1uOYVQXKpQKDkK435ZxKt5c6DFulfxfa7","timestamp":1703576725184},{"file_id":"https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb","timestamp":1676885530373}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
